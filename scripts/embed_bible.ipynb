{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment and check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Huggingface embedding model: hfl/chinese-roberta-wwm-ext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lchai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No sentence-transformers model found with name hfl/chinese-roberta-wwm-ext. Creating a new one with mean pooling.\n",
      "c:\\Users\\Lchai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "c:\\Users\\Lchai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml \n",
    "import torch \n",
    "\n",
    "# Bible data directory \n",
    "data_directory = \"../data/bible_versions/cuvs\"\n",
    "assert(os.path.isdir(data_directory)), f\"Data directory does not exist...\"\n",
    "\n",
    "# Load the embedding config file \n",
    "config_file = \"./bible_embedding_config.yaml\"\n",
    "assert(os.path.isfile(config_file)), \"Embedding config file does not exist...\"\n",
    "\n",
    "with open(config_file, \"r\") as f: \n",
    "    config = yaml.safe_load(f) \n",
    "\n",
    "# Load embedding model \n",
    "assert(\"model\" in config), f\"'model' configuration missed...\"\n",
    "\n",
    "if (config[\"model\"][\"framework\"] == \"huggingface\"): \n",
    "    model_name = config[\"model\"][\"name\"] \n",
    "    print(f\"Loading Huggingface embedding model: {model_name}\")\n",
    "\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings \n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "    embedder = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\n",
    "            \"device\": (\n",
    "                \"cuda\" \n",
    "                if (torch.cuda.is_available()) \n",
    "                else \"cpu\"\n",
    "            )\n",
    "        },\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": False\n",
    "        }\n",
    "    )\n",
    "else: \n",
    "    assert(False), f\"Unknown embedding framework: {config['model']['framework']}\"\n",
    "\n",
    "# Probe the embedding dimension \n",
    "embedding_dim = len(embedder.embed_query(\"this is a test\")) \n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Load the vector store (client) \n",
    "vs_provider = config[\"vector_store\"][\"provider\"]\n",
    "\n",
    "if (vs_provider == \"qdrant\"): \n",
    "    # Create Qdrant client \n",
    "    from qdrant_client import QdrantClient \n",
    "    qdrant_client = QdrantClient(**config[\"vector_store\"][\"client_args\"])\n",
    "\n",
    "    # Create the collection if it does not exist \n",
    "    from qdrant_client.models import VectorParams, Distance\n",
    "    collection_name = config[\"vector_store\"][\"collection_name\"]\n",
    "\n",
    "    if (not qdrant_client.collection_exists(collection_name)): \n",
    "        qdrant_client.create_collection(\n",
    "            collection_name, \n",
    "            vectors_config=VectorParams(\n",
    "                size=embedding_dim, \n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Wrap the Qdrant client with LangChain \n",
    "    from langchain_qdrant import QdrantVectorStore \n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=collection_name, \n",
    "        embedding=embedder\n",
    "    )\n",
    "\n",
    "else: \n",
    "    assert(False), f\"Unsupported vector store provider: {vs_provider}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing book genesis...: 100%|██████████| 1533/1533 [12:54<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "from tqdm import tqdm \n",
    "from pathlib import Path \n",
    "from langchain_core.documents import Document \n",
    "from bible_study_bot.data.loaders import load_bible_book_from_file, load_verse_context\n",
    "from bible_study_bot.data.preprocesses import preproc_for_embedding\n",
    "\n",
    "for book_name, book_params in config[\"books\"].items():\n",
    "    # Check if the book file exist\n",
    "    book_file = Path(data_directory, book_params[\"file\"])\n",
    "    assert(book_file.exists()), f\"Book file does not exist: {book_file}\"\n",
    "    assert(book_file.suffix in [\".yaml\", \".yml\"]), f\"Unsupported book file format: {book_file.suffix}\"\n",
    "\n",
    "    # Load embedding parameters \n",
    "    embedding_context_scope = book_params[\"context_scope\"] \n",
    "    n_prev_context_verses = (\n",
    "        config[\"defaults\"][\"n_prev_context_verses\"]\n",
    "        if (\"n_prev_context_verses\" not in book_params)\n",
    "        else book_params[\"n_prev_context_verses\"]\n",
    "    )\n",
    "    n_next_context_verses = (\n",
    "        config[\"defaults\"][\"n_next_context_verses\"]\n",
    "        if (\"n_next_context_verses\" not in book_params)\n",
    "        else book_params[\"n_next_context_verses\"]\n",
    "    )\n",
    "\n",
    "    # Load the book file \n",
    "    bible_book = load_bible_book_from_file(book_file)\n",
    "\n",
    "    # Iterate through the verses \n",
    "    for bible_verse in tqdm(bible_book.verses, desc=f\"Processing book {book_name}...\", total=len(bible_book.verses)): \n",
    "        verse_context_text = load_verse_context(\n",
    "            bible_book=bible_book, \n",
    "            chapter=bible_verse.metadata[\"chapter\"], \n",
    "            verse=bible_verse.metadata[\"verse\"], \n",
    "            context_scope=embedding_context_scope, \n",
    "            n_prev_context_verses=n_prev_context_verses, \n",
    "            n_next_context_verses=n_next_context_verses\n",
    "        )\n",
    "\n",
    "        verse_context_text = preproc_for_embedding(verse_context_text)\n",
    "        \n",
    "        # Save the text to vector store \n",
    "        vector_store.add_documents(\n",
    "            documents=[Document(\n",
    "                page_content=verse_context_text, \n",
    "                metadata={\n",
    "                    \"text\": bible_verse.text, \n",
    "                    **bible_book.metadata, \n",
    "                    **bible_verse.metadata\n",
    "                }\n",
    "            )], \n",
    "            ids=[str(uuid.uuid4())]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rels = vector_store.similarity_search(\n",
    "#     \"谁进入了挪亚的方舟？\",\n",
    "#     k=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
